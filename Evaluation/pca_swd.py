import os
import pandas as pd
import numpy as np
import random
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from EMD import getEMD
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
import cg_tm_kl2
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import ot
colors = ['blue', 'orange']
marker = ['.', '.']

def getEMD(xs, xt, num_samples, path,filename):
    n = num_samples  # Number of samples

    # Uniform distribution on samples
    a, b = np.ones((n,)), np.ones((n,))

    # æºå’Œç›®æ ‡åˆ†å¸ƒå›¾
    plt.figure(figsize=(8, 6))
    plt.scatter(xs[:, 0], xs[:, 1], color='blue', marker='+', label='Source samples', s=40, alpha=0.7)
    plt.scatter(xt[:, 0], xt[:, 1], color='red', marker='x', label='Target samples', s=40, alpha=0.7)
    plt.legend(loc='best', fontsize=10, frameon=True, fancybox=True, shadow=True)
    plt.title('Source and Target Distributions', fontsize=14, fontweight='bold')
    plt.xlabel('X-axis', fontsize=12)
    plt.ylabel('Y-axis', fontsize=12)
    plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)

    # ä¿å­˜æ–‡ä»¶
    savename = os.path.splitext(os.path.basename(path))[0]
    # save_dir = 'PICC'
    save_dir = f'/home/fan/Code4idea/xLSTMstega/results/{filename}/PCA'
    os.makedirs(save_dir, exist_ok=True)
    plt.savefig(os.path.join(save_dir, f'{savename}_distribution.jpg'))
    plt.close()

    # è®¡ç®— SWD
    n_seed = 50
    n_projections_arr = np.logspace(0, 3, 25, dtype=int)
    res = np.empty((n_seed, 25))
    for seed in range(n_seed):
        for i, n_projections in enumerate(n_projections_arr):
            res[seed, i] = ot.sliced_wasserstein_distance(xs, xt, a, b, n_projections, seed=seed)

    res_mean = np.mean(res, axis=0)
    res_std = np.std(res, axis=0)

    # ç»˜åˆ¶ SWD æ›²çº¿ï¼ˆä¿æŒä¸å˜ï¼‰
    plt.figure(2)
    plt.plot(n_projections_arr, res_mean, label="SWD")
    plt.fill_between(n_projections_arr, res_mean - 2 * res_std, res_mean + 2 * res_std, alpha=0.5)
    plt.legend()
    plt.xscale('log')
    plt.xlabel("Number of projections")
    plt.ylabel("Distance")
    plt.title('Sliced Wasserstein Distance with 95% confidence interval')

    return res_mean[-1]

def split_words(line, num):
    words = [line[i:i+num] for i in range(0, len(line), num)]
    return words

def total_vector(words, word2vec):
    vec = np.zeros(300).reshape((1, 300))
    for word in words:
        try:
            vec += word2vec.wv[word].reshape((1, 300))
        except KeyError:
            continue
    return vec

def process_sequence(path, word2vec, split_length, seq_length, base_name=None, max_lines=None):
    """
    å¤„ç†åºåˆ—æ•°æ®ï¼Œæ ¹æ®æ–‡ä»¶è¡Œæ•°è‡ªé€‚åº”å¤„ç†
    
    Args:
        path: æ–‡ä»¶è·¯å¾„
        word2vec: Word2Vecæ¨¡å‹
        split_length: åˆ†å‰²é•¿åº¦
        seq_length: åºåˆ—é•¿åº¦
        base_name: åŸºç¡€åç§°ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦æ˜¯ASM286374v1ï¼‰
        max_lines: æœ€å¤§å¤„ç†è¡Œæ•°
    """
    with open(path, 'r') as f:
        lines = f.readlines()
    
    # å¦‚æœæŒ‡å®šäº†æœ€å¤§è¡Œæ•°ï¼Œåˆ™é™åˆ¶å¤„ç†è¡Œæ•°
    if max_lines and len(lines) > max_lines:
        lines = lines[:max_lines]
        print(f"é™åˆ¶å¤„ç†æ–‡ä»¶ {path} çš„å‰ {max_lines} è¡Œ")
    
    print(f"å¤„ç†æ–‡ä»¶ {path}ï¼Œå…± {len(lines)} è¡Œ")
    
    # å¤„ç†åºåˆ—
    raw_pos = cg_tm_kl2.txt_process_sc_duo(path, len_sc=seq_length, beg_sc=0, end_sc=len(lines), 
                                         PADDING=False, flex=10, num1=split_length, tiqu=False)
    raw_pos = list(filter(lambda x: x not in ['', None], raw_pos))
    
    print(f"å¤„ç†åå¾—åˆ° {len(raw_pos)} ä¸ªæœ‰æ•ˆåºåˆ—")

    pos_df = pd.DataFrame(raw_pos, columns=[0])
    pos_df['words'] = pos_df[0].apply(lambda x: split_words(x, split_length))

    vectors = [total_vector(words, word2vec) for words in pos_df['words']]
    return np.squeeze(vectors)


def plot_with_labels(embedded_data, labels, path, output_dir):
    plt.cla()
    fig, ax = plt.subplots()
    ax.axis("off")

    data_df = pd.DataFrame({'x': embedded_data[:, 0], 'y': embedded_data[:, 1], 'label': labels})
    for index in [0, 1]:
        subset = data_df[data_df['label'] == index]
        plt.scatter(subset['x'], subset['y'], marker=marker[index], color=colors[index], alpha=0.65)
    # å°† .csv æ–‡ä»¶è·¯å¾„è½¬æ¢ä¸º .jpg æ–‡ä»¶è·¯å¾„
    base_filename = path.replace('.csv', '.jpg')
    savename = os.path.join(output_dir, base_filename)
    plt.savefig(savename)

def save_emd_result(pd_EMD, emd_result_path):
    """ä¿å­˜EMDç»“æœï¼Œé¿å…æ•°æ®æ ¼å¼é—®é¢˜"""
    try:
        # ç¡®ä¿EMDæ•°æ®æ˜¯æ•°å€¼ç±»å‹
        pd_EMD['emd'] = pd.to_numeric(pd_EMD['emd'], errors='coerce')

        # ç§»é™¤æ— æ•ˆæ•°æ®
        pd_EMD = pd_EMD.dropna(subset=['emd'])

        if len(pd_EMD) == 0:
            print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„EMDæ•°æ®éœ€è¦ä¿å­˜")
            return

        # å¦‚æœæ–‡ä»¶å·²ç»å­˜åœ¨ï¼Œè¯»å–ç°æœ‰æ–‡ä»¶å¹¶åˆå¹¶æ•°æ®
        if os.path.exists(emd_result_path):
            try:
                existing_df = pd.read_csv(emd_result_path)
                # ç¡®ä¿ç°æœ‰æ•°æ®çš„emdåˆ—ä¹Ÿæ˜¯æ•°å€¼ç±»å‹
                existing_df['emd'] = pd.to_numeric(existing_df['emd'], errors='coerce')
                existing_df = existing_df.dropna(subset=['emd'])

                # åˆå¹¶æ•°æ®
                combined_df = pd.concat([existing_df, pd_EMD], ignore_index=True)
                # å»é‡ï¼ˆåŸºäºnameåˆ—ï¼‰
                combined_df = combined_df.drop_duplicates(subset=['name'], keep='last')
                combined_df.to_csv(emd_result_path, mode='w', header=True, index=False)
                print(f"   åˆå¹¶ä¿å­˜ {len(combined_df)} æ¡EMDè®°å½•")
            except Exception as e:
                print(f"âš ï¸  è¯»å–ç°æœ‰EMDæ–‡ä»¶æ—¶å‡ºé”™: {e}")
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œç›´æ¥è¦†ç›–æ–‡ä»¶
                pd_EMD.to_csv(emd_result_path, mode='w', header=True, index=False)
                print(f"   é‡æ–°åˆ›å»ºEMDæ–‡ä»¶ï¼Œä¿å­˜ {len(pd_EMD)} æ¡è®°å½•")
        else:
            # æ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
            pd_EMD.to_csv(emd_result_path, mode='w', header=True, index=False)
            print(f"   åˆ›å»ºæ–°EMDæ–‡ä»¶ï¼Œä¿å­˜ {len(pd_EMD)} æ¡è®°å½•")

    except Exception as e:
        print(f"âŒ ä¿å­˜EMDç»“æœæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    # files_name = ['ASM141792v1', 'ASM286374v1']  # ä¿®æ”¹ä¸ºå®é™…éœ€è¦å¤„ç†çš„æ–‡ä»¶
    # files_name = ['ASM141792v1']
    files_name = ['ASM141792v1', 'ASM286374v1', 'ASM400647v1', 'ASM949793v1']
    read_files = []
    # files_name = ['ASM141792v1', 'ASM286374v1', 'ASM400647v1', 'ASM949793v1', 'ASM1821919v1']
    asm_to_gca = {
        'ASM141792v1': 'GCA_001417925',
        'ASM286374v1': 'GCA_002863745',
        'ASM400647v1': 'GCA_004006475',
        'ASM949793v1': 'GCA_009497935',
        'ASM1821919v1': 'GCA_018219195'
    }
    for base_name in files_name:
        base_dir = f'/home/fan/Code4idea/xLSTMstega/Dataset'
        for file in os.listdir(base_dir):
            # åªå¤„ç†ä¸å½“å‰base_nameåŒ¹é…çš„æ–‡ä»¶
            if file.endswith('.txt') and file.startswith(f'{base_name}_'):
                file_path = os.path.join(base_dir, file)
                read_files.append(file_path)
                
                try:
                    length_str = file.split('_')[1]  # æå–é•¿åº¦å­—ç¬¦ä¸²
                    devide_ = int(file.split('_')[2].split('.')[0])  # æå–åˆ†å—å‚æ•°
                    len_sc = 198 if length_str == '198' else 200  # ç¡®å®šåºåˆ—é•¿åº¦
                except (IndexError, ValueError):
                    raise ValueError(f"æ–‡ä»¶å {file} ä¸ç¬¦åˆé¢„æœŸæ ¼å¼ï¼Œæ— æ³•æå–é•¿åº¦å’Œåˆ†å‰²å‚æ•°ã€‚")
                
                # åˆ›å»ºæ¨¡å‹æ–‡ä»¶å - ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„base_name
                model_name = f"{base_name}_{len_sc}_{devide_}.model"
                embedding_model_path = os.path.join(base_dir, model_name)

                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
                if os.path.exists(embedding_model_path):
                    # åŠ è½½å·²å­˜åœ¨çš„æ¨¡å‹
                    word2vec_model = Word2Vec.load(embedding_model_path)
                    print(f"å·²åŠ è½½æ¨¡å‹: {embedding_model_path}")
                else:
                    # ç”Ÿæˆæ–°æ¨¡å‹å¹¶ä¿å­˜
                    print(f"è®­ç»ƒæ–°æ¨¡å‹: {embedding_model_path}")
                    all_sequences = [split_words(line.strip(), devide_) for line in open(file_path, 'r')]
                    word2vec_model = Word2Vec(all_sequences, vector_size=300, window=devide_, min_count=5, sg=1, hs=1, epochs=10)
                    word2vec_model.save(embedding_model_path)
                
                # ä»…å¤„ç†ä¸å½“å‰æ¨¡å‹åŒ¹é…çš„æ•°æ®æ–‡ä»¶
                if f"_{len_sc}_{devide_}" in file:
                    print(f"å¤„ç†æ–‡ä»¶: {file_path} ä½¿ç”¨æ¨¡å‹: {model_name}")
                    # å¤„ç†å½“å‰æ•°æ®æ–‡ä»¶
                    
                    # å¤„ç†åŸå§‹åºåˆ—
                    original_vectors = process_sequence(file_path, word2vec_model, 
                                                     split_length=devide_, 
                                                     seq_length=len_sc,
                                                     base_name=base_name,
                                                     max_lines=None)
                    original_2D = PCA(n_components=2).fit_transform(original_vectors)

                    # éšå†™æ–‡ä»¶è·¯å¾„åˆ—è¡¨ - ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„base_name
                    steganography_dirs = [
                        f'/home/fan/Code4idea/xLSTMstega/Stego_DNA/{base_name}',
                        f'/home/fan/Code4idea/xLSTMstega/Stego_DNA/Baselines/{base_name}'
                    ]

                    # åˆ›å»ºç»“æœå­˜å‚¨ç›®å½•
                    base_results_dir = os.path.join('/home/fan/Code4idea/xLSTMstega/results', base_name)
                    csv_dir = os.path.join(base_results_dir, 'CSV')
                    jpg_dir = os.path.join(base_results_dir, 'JPG')
                    os.makedirs(csv_dir, exist_ok=True)
                    os.makedirs(jpg_dir, exist_ok=True)

                    # åˆ›å»ºä¸€ä¸ª DataFrame æ¥å­˜å‚¨ EMD ç»“æœ
                    pd_EMD = pd.DataFrame(columns=['name', 'emd'])

                    # å¤„ç†æ¯ä¸ªç›®å½•ä¸‹çš„éšå†™æ–‡ä»¶
                    for steganography_dir in steganography_dirs:
                        if not os.path.exists(steganography_dir):
                            print(f"è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡ï¼š{steganography_dir}")
                            continue
                            
                        print(f"å¤„ç†ç›®å½•ï¼š{steganography_dir}")
                        # å¤„ç†éšå†™æ–‡ä»¶
                        steganography_files = [f for f in os.listdir(steganography_dir) if f.endswith('.txt')]
                        for stego_file in steganography_files:
                            # ç¡®ä¿éšå†™æ–‡ä»¶ä¸å½“å‰å¤„ç†çš„åŸå§‹æ–‡ä»¶å‚æ•°åŒ¹é…
                            if f"_{len_sc}_{devide_}" in stego_file:
                                # æ£€æŸ¥å¯¹åº”çš„è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                                csv_filename = stego_file.replace('.txt', '_PCA.csv')
                                jpg_filename = stego_file.replace('.txt', '_PCA.jpg')
                                csv_path = os.path.join(csv_dir, csv_filename)
                                jpg_path = os.path.join(jpg_dir, jpg_filename)
                                
                                # å¦‚æœè¾“å‡ºæ–‡ä»¶éƒ½å·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡æ­¤æ–‡ä»¶
                                if os.path.exists(csv_path) and os.path.exists(jpg_path):
                                    print(f"æ–‡ä»¶ {stego_file} çš„å¤„ç†ç»“æœå·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†")
                                    continue

                                try:
                                    stego_path = os.path.join(steganography_dir, stego_file)
                                    print(f"å¤„ç†éšå†™æ–‡ä»¶: {stego_path}")
                                    
                                    # è·å–éšå†™æ–‡ä»¶çš„è¡Œæ•°
                                    with open(stego_path, 'r') as f:
                                        stego_lines = len(f.readlines())
                                    
                                    # æ ¹æ®éšå†™æ–‡ä»¶çš„è¡Œæ•°ï¼Œé‡æ–°å¤„ç†åŸå§‹æ–‡ä»¶
                                    if stego_lines < len(original_vectors):
                                        print(f"éšå†™æ–‡ä»¶ {stego_file} è¡Œæ•° ({stego_lines}) å°‘äºåŸå§‹å‘é‡æ•° ({len(original_vectors)})")
                                        print(f"å°†ä½¿ç”¨å‰ {stego_lines} ä¸ªåŸå§‹å‘é‡è¿›è¡Œæ¯”è¾ƒ")
                                        original_vectors_subset = original_vectors[:stego_lines]
                                        original_2D_subset = PCA(n_components=2).fit_transform(original_vectors_subset)
                                    else:
                                        original_vectors_subset = original_vectors
                                        original_2D_subset = original_2D
                                    
                                    # å¤„ç†éšå†™æ–‡ä»¶
                                    stego_vectors = process_sequence(stego_path, word2vec_model, 
                                                                  split_length=devide_, 
                                                                  seq_length=len_sc,
                                                                  base_name=base_name,
                                                                  max_lines=None)  # ä¸é™åˆ¶éšå†™æ–‡ä»¶çš„è¡Œæ•°
                                    
                                    # ç¡®ä¿ä¸¤ä¸ªå‘é‡é›†åˆçš„å¤§å°ç›¸åŒ
                                    min_size = min(len(original_vectors_subset), len(stego_vectors))
                                    if min_size == 0:
                                        print(f"è­¦å‘Šï¼šå¤„ç†åçš„å‘é‡ä¸ºç©ºï¼Œè·³è¿‡æ–‡ä»¶ {stego_file}")
                                        continue
                                        
                                    if len(original_vectors_subset) != len(stego_vectors):
                                        print(f"è°ƒæ•´å‘é‡å¤§å°ï¼šåŸå§‹ {len(original_vectors_subset)} -> {min_size}ï¼Œéšå†™ {len(stego_vectors)} -> {min_size}")
                                        original_vectors_subset = original_vectors_subset[:min_size]
                                        original_2D_subset = original_2D_subset[:min_size] if len(original_2D_subset) > min_size else original_2D_subset
                                        stego_vectors = stego_vectors[:min_size]
                                    
                                    # å¯¹éšå†™å‘é‡è¿›è¡ŒPCAé™ç»´
                                    stego_2D = PCA(n_components=2).fit_transform(stego_vectors)
                                    
                                    # è®¡ç®—EMD
                                    emd_value = getEMD(original_2D_subset, stego_2D, len(original_2D_subset), stego_path, base_name)
                                    emd_row = pd.DataFrame({'name': [stego_file], 'emd': [emd_value]})
                                    pd_EMD = pd.concat([pd_EMD, emd_row], ignore_index=True)

                                    # ä¿å­˜åµŒå…¥ç»“æœåˆ°CSVç›®å½•
                                    combined_df = pd.DataFrame({
                                        'ori_x': original_2D_subset[:, 0], 'ori_y': original_2D_subset[:, 1],
                                        'stego_x': stego_2D[:, 0], 'stego_y': stego_2D[:, 1]
                                    })
                                    combined_df.to_csv(csv_path, index=False)

                                    # ç»˜åˆ¶PCAé™ç»´å›¾å¹¶ä¿å­˜åˆ°JPGç›®å½•
                                    plot_with_labels(original_2D_subset, np.zeros(len(original_2D_subset)), jpg_path, jpg_dir)
                                    plot_with_labels(stego_2D, np.ones(len(stego_2D)), jpg_path, jpg_dir)
                                    print(f"å®Œæˆ {stego_file} çš„å¤„ç†")
                                except Exception as e:
                                    print(f"é”™è¯¯å‘ç”Ÿåœ¨æ–‡ä»¶: {stego_path}")
                                    print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                                    print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
                                    import traceback
                                    traceback.print_exc()
                                    continue  # è·³è¿‡è¿™ä¸ªæ–‡ä»¶ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                    # ä¿å­˜EMDç»“æœ
                    emd_result_path = os.path.join(base_results_dir, f'{asm_to_gca[base_name]}_emd_results.csv')
                    save_emd_result(pd_EMD, emd_result_path)
                    print(f"{base_name} çš„æ‰€æœ‰EMDç»“æœå·²ä¿å­˜åˆ° {emd_result_path}ã€‚")

                    # é‡æ–°è¯»å–å®Œæ•´çš„EMDç»“æœæ–‡ä»¶å¹¶è®¡ç®—ç»Ÿè®¡
                    print(f"ğŸ”„ é‡æ–°è¯»å–EMDç»“æœæ–‡ä»¶è¿›è¡Œç»Ÿè®¡è®¡ç®—...")
                    if os.path.exists(emd_result_path):
                        try:
                            # è¯»å–å®Œæ•´çš„EMDç»“æœæ–‡ä»¶
                            full_emd_df = pd.read_csv(emd_result_path)
                            print(f"   è¯»å–åˆ° {len(full_emd_df)} æ¡EMDè®°å½•")

                            # æ£€æŸ¥å¹¶æ¸…ç†æ•°æ®ç±»å‹é—®é¢˜
                            print(f"   æ£€æŸ¥EMDæ•°æ®ç±»å‹...")
                            # ç¡®ä¿emdåˆ—æ˜¯æ•°å€¼ç±»å‹
                            full_emd_df['emd'] = pd.to_numeric(full_emd_df['emd'], errors='coerce')

                            # ç§»é™¤æ— æ•ˆæ•°æ®
                            invalid_count = full_emd_df['emd'].isna().sum()
                            if invalid_count > 0:
                                print(f"   âš ï¸  å‘ç° {invalid_count} æ¡æ— æ•ˆEMDæ•°æ®ï¼Œå°†è¢«ç§»é™¤")
                                full_emd_df = full_emd_df.dropna(subset=['emd'])
                                print(f"   æ¸…ç†åå‰©ä½™ {len(full_emd_df)} æ¡æœ‰æ•ˆè®°å½•")

                            # æå–åŸºç¡€æ–¹æ³•å
                            full_emd_df['base_name'] = full_emd_df['name'].apply(lambda x: '_'.join(x.split('_')[:-1]))

                            # é‡æ–°è®¡ç®—æ‰€æœ‰æ–¹æ³•çš„ç»Ÿè®¡ç»“æœ
                            stats = []
                            for name in sorted(full_emd_df['base_name'].unique()):
                                group = full_emd_df[full_emd_df['base_name'] == name]
                                if len(group) > 0:
                                    print(f'   ç»Ÿè®¡æ–¹æ³• {name}: {len(group)} ä¸ªæ ·æœ¬')
                                    mean_val = group['emd'].mean()
                                    std_val = group['emd'].std()
                                    stats_dict = {
                                        'method': name,
                                        'EMD': f"{mean_val:.2f}Â±{std_val:.2f}"
                                    }
                                    stats.append(stats_dict)

                            # åˆ›å»ºç»Ÿè®¡ç»“æœDataFrameå¹¶å®Œå…¨é‡å†™ç»Ÿè®¡æ–‡ä»¶
                            stats_df = pd.DataFrame(stats)
                            stats_writefile = os.path.join(base_results_dir, f'{asm_to_gca[base_name]}_emd_stats.csv')
                            print(f"âœ… é‡æ–°ç”ŸæˆEMDç»Ÿè®¡ç»“æœåˆ°ï¼š{stats_writefile}")
                            # å®Œå…¨é‡å†™ç»Ÿè®¡æ–‡ä»¶ï¼ˆä¸è¿½åŠ ï¼‰
                            stats_df.to_csv(stats_writefile, mode='w', header=True, index=False)
                            print(f"   ç»Ÿè®¡æ–‡ä»¶åŒ…å« {len(stats_df)} ä¸ªæ–¹æ³•çš„ç»Ÿè®¡ç»“æœ")

                        except Exception as e:
                            print(f"âŒ å¤„ç†EMDç»Ÿè®¡æ—¶å‡ºé”™: {e}")
                            print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
                            import traceback
                            traceback.print_exc()
                    else:
                        print(f"âš ï¸  EMDç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {emd_result_path}")
